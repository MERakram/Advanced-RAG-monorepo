# üöÄ Advanced RAG System

A production-ready **Retrieval-Augmented Generation (RAG)** platform featuring local LLM inference, hybrid retrieval, multi-agent orchestration, semantic caching, and full observability.

![Architecture](https://img.shields.io/badge/Architecture-Microservices-blue)
![Python](https://img.shields.io/badge/Python-3.11+-green)
![Docker](https://img.shields.io/badge/Docker-Compose-blue)
![License](https://img.shields.io/badge/License-MIT-yellow)

---

## ‚ú® Features

- **ü§ñ Local LLM Inference** - Run models locally with vLLM (no API costs for development)
- **üåê Cloud LLM Fallback** - Route complex queries to OpenRouter (Claude, GPT-4, etc.)
- **üîç Hybrid Retrieval** - Dense + Sparse vector search with Qdrant
- **üìä Full Observability** - Langfuse tracing with session & user tracking
- **üíæ Semantic Caching** - Instant responses for similar queries
- **üìÑ Multi-Format Ingestion** - PDF, DOCX, HTML, Markdown (+ OCR for images)
- **üéØ OpenAI-Compatible API** - Drop-in replacement for the OpenAI API

---

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                              Open WebUI                                ‚îÇ
‚îÇ                           (localhost:3000)                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
                                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         RAG Backend (FastAPI)                          ‚îÇ
‚îÇ                           (localhost:5002)                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ  Semantic   ‚îÇ  ‚îÇ   Query     ‚îÇ  ‚îÇ  Re-Ranker  ‚îÇ  ‚îÇ   Model     ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ   Cache     ‚îÇ  ‚îÇ  Rewriting  ‚îÇ  ‚îÇ  (Cross-Enc)‚îÇ  ‚îÇ   Router    ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ                   ‚îÇ                              ‚îÇ
           ‚ñº                   ‚ñº                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     Qdrant      ‚îÇ  ‚îÇ      vLLM       ‚îÇ           ‚îÇ    OpenRouter API   ‚îÇ
‚îÇ  (Vector DB)    ‚îÇ  ‚îÇ  (Local LLM)    ‚îÇ           ‚îÇ   (Cloud Fallback)  ‚îÇ
‚îÇ  localhost:6333 ‚îÇ  ‚îÇ  localhost:9999 ‚îÇ           ‚îÇ                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         Observability Stack                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ  Langfuse   ‚îÇ  ‚îÇ ClickHouse  ‚îÇ  ‚îÇ    MinIO    ‚îÇ  ‚îÇ    Redis    ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  (UI:3001)  ‚îÇ  ‚îÇ   (OLAP)    ‚îÇ  ‚îÇ    (S3)     ‚îÇ  ‚îÇ   (Queue)   ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üê≥ Docker Services

| Container             | Image                           | Port      | Purpose                    |
| --------------------- | ------------------------------- | --------- | -------------------------- |
| `rag-open-webui`      | `ghcr.io/open-webui/open-webui` | 3000      | Chat UI (like ChatGPT)     |
| `rag-backend`         | Custom (Dockerfile)             | 5002      | FastAPI RAG orchestrator   |
| `rag-vllm`            | `vllm/vllm-openai`              | 9999      | Local LLM inference        |
| `rag-qdrant`          | `qdrant/qdrant`                 | 6333      | Vector database            |
| `rag-langfuse`        | `langfuse/langfuse:3`           | 3001      | Observability UI           |
| `rag-langfuse-worker` | `langfuse/langfuse-worker:3`    | 3030      | Trace processing           |
| `rag-clickhouse`      | `clickhouse/clickhouse-server`  | 18123     | Trace storage (OLAP)       |
| `rag-minio`           | `minio/minio`                   | 9090/9091 | S3-compatible blob storage |
| `rag-redis`           | `redis:7.2`                     | 6379      | Queue & cache              |
| `rag-langfuse-db`     | `postgres:16`                   | -         | Langfuse metadata DB       |

---

## üöÄ Quick Start

### Prerequisites

- **Docker Desktop** (with GPU support for vLLM)
- **NVIDIA GPU** (Recommended, 8GB+ VRAM)
- **Git**

### 1. Clone & Configure

```bash
git clone https://github.com/yourusername/Advanced-RAG.git
cd Advanced-RAG

# Copy environment template
cp .env.example .env
# Edit .env with your API keys (OpenRouter, etc.)
```

### 2. Start All Services

```bash
docker compose up -d
```

### 3. Access the UI

- **Chat UI**: http://localhost:3000 (Open WebUI)
- **Langfuse Dashboard**: http://localhost:3001
- **API Docs**: http://localhost:5002/docs

### Default Langfuse Credentials

- Email: `admin@rag.local`
- Password: `ragadmin123`

---

## ‚öôÔ∏è Environment Variables

| Variable             | Description                                | Default                      |
| -------------------- | ------------------------------------------ | ---------------------------- |
| `OPENROUTER_API_KEY` | API key for cloud LLM fallback             | Required for cloud models    |
| `LOCAL_MODEL_NAME`   | Model to run with vLLM                     | `Qwen/Qwen2.5-0.5B-Instruct` |
| `ENABLE_OCR`         | Enable OCR for image files (GPU intensive) | `false`                      |
| `LANGFUSE_DEBUG`     | Enable Langfuse debug logging              | `false`                      |
| `WEBUI_SECRET_KEY`   | Secret for Open WebUI sessions             | Set in compose               |

See `.env.example` for the full list.

---

## üìÅ Project Structure

```
Advanced-RAG/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ main.py                 # FastAPI app & endpoints
‚îÇ   ‚îú‚îÄ‚îÄ config.py               # Model & provider configuration
‚îÇ   ‚îú‚îÄ‚îÄ ingestion/              # Document processing pipeline
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ router.py           # Ingestion orchestrator
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ docling_parser.py   # PDF/DOCX parser
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ deepseek_ocr.py     # OCR for images (optional)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metadata.py         # LLM-based metadata extraction
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chunking.py         # Hierarchical chunking
‚îÇ   ‚îú‚îÄ‚îÄ retrieval/              # Search & retrieval
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ engine.py           # Query rewriting, HyDE
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ qdrant_client.py    # Vector DB operations
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ reranker.py         # Cross-encoder reranking
‚îÇ   ‚îú‚îÄ‚îÄ generation/             # Response generation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agents.py           # Multi-agent orchestration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ router.py           # Model routing (local/cloud)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ semantic_cache.py   # Query caching
‚îÇ   ‚îî‚îÄ‚îÄ observability/          # Monitoring
‚îÇ       ‚îî‚îÄ‚îÄ config.py           # Langfuse setup
‚îú‚îÄ‚îÄ docker-compose.yml          # All services
‚îú‚îÄ‚îÄ Dockerfile                  # RAG backend image
‚îú‚îÄ‚îÄ pyproject.toml              # Python dependencies
‚îî‚îÄ‚îÄ requirements.txt            # Pip dependencies
```

---

## üîÑ How It Works

### Ingestion Pipeline (Upload a Document)

1. **File Detection** ‚Üí Route to Docling (PDF/DOCX) or OCR (images)
2. **Text Extraction** ‚Üí Preserve structure (tables, headers)
3. **Metadata Enrichment** ‚Üí LLM extracts department, date, summary
4. **Hierarchical Chunking** ‚Üí Parent (1024 tok) + Child (256 tok) chunks
5. **Vector Upsert** ‚Üí Dense + Sparse embeddings to Qdrant

### Query Pipeline (Ask a Question)

1. **Semantic Cache Check** ‚Üí Return cached answer if similarity > 0.95
2. **Query Rewriting** ‚Üí Expand ambiguous queries
3. **Hybrid Search** ‚Üí Dense (semantic) + Sparse (keyword) in Qdrant
4. **Re-ranking** ‚Üí Cross-encoder scores top 50 ‚Üí keep top 5
5. **Model Routing** ‚Üí Simple ‚Üí Local vLLM, Complex ‚Üí OpenRouter
6. **Response Generation** ‚Üí Stream answer with context
7. **Cache Update** ‚Üí Store Q&A for future queries

---

## üìä Observability (Langfuse)

Access the Langfuse dashboard at http://localhost:3001

### Features

- **Traces** - Full execution path for each request
- **Sessions** - Group traces by conversation (chat thread)
- **Users** - Track usage per user
- **Costs** - Token usage and cost breakdown
- **Scores** - User feedback (thumbs up/down)

### Session Tracking

Open WebUI automatically sends session headers when `ENABLE_OPENWEBUI_USER_HEADERS=true`:

- `X-OpenWebUI-Chat-Id` ‚Üí Groups all messages in a conversation
- `X-OpenWebUI-User-Id` ‚Üí Links traces to users

---

## üõ†Ô∏è Development

### Running Locally (without Docker)

```bash
# Install dependencies
pip install poetry
poetry install

# Start backend
poetry run uvicorn src.main:app --reload --port 8000
```

### Poetry Convenience Scripts

After running `poetry install`, you can use these commands to manage Docker services:

| Command                 | Description                                   |
| ----------------------- | --------------------------------------------- |
| `poetry run llm-up`     | Start vLLM service (local LLM inference)      |
| `poetry run llm-down`   | Stop vLLM service                             |
| `poetry run trace-up`   | Start Langfuse + dependencies (observability) |
| `poetry run trace-down` | Stop Langfuse + dependencies                  |
| `poetry run app-up`     | Start RAG backend + Qdrant + Open WebUI       |
| `poetry run app-build`  | Rebuild and start the RAG backend             |
| `poetry run app-down`   | Stop RAG backend + Qdrant + Open WebUI        |
| `poetry run start-all`  | üöÄ Start all services                         |
| `poetry run stop-all`   | Stop all services                             |
| `poetry run status`     | Show status of all containers                 |

**Example workflow:**

```bash
# Start everything at once
poetry run start-all

# Or start services individually
poetry run trace-up      # Start observability first
poetry run llm-up        # Start local LLM
poetry run app-up        # Start the RAG application

# Check what's running
poetry run status

# Stop everything
poetry run stop-all
```

### Adding New Models

Edit `src/config.py` to add new models:

```python
ModelConfig(
    id="your-model-id",
    name="Display Name",
    provider=Provider.OPENROUTER,  # or Provider.VLLM
    context_window=8192,
)
```

---

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing`)
5. Open a Pull Request

---

## üìù License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üôè Acknowledgments

- [LlamaIndex](https://www.llamaindex.ai/) - RAG framework
- [vLLM](https://github.com/vllm-project/vllm) - Fast LLM inference
- [Qdrant](https://qdrant.tech/) - Vector database
- [Langfuse](https://langfuse.com/) - LLM observability
- [Open WebUI](https://openwebui.com/) - Chat interface
- [Docling](https://github.com/DS4SD/docling) - Document parsing
